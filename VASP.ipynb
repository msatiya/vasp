{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suitable-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vojta/tf/lib/python3.6/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.5.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "little-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalToZero(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        \"\"\"Set diagonal to zero\"\"\"\n",
    "        q = tf.linalg.set_diag(w, tf.zeros(w.shape[0:-1]), name=None)\n",
    "        return q\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a basket.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VASP(Model):\n",
    "    class Model(tf.keras.Model):\n",
    "        def __init__(self, num_words, latent=1024, hidden=1024, items_sampling=1.):\n",
    "            \"\"\"\n",
    "            num_words             nr of items in dataset (size of tokenizer)\n",
    "            latent                size of latent space\n",
    "            hidden                size of hidden layers\n",
    "            items_sampling        Large items datatsets can be very gpu memory consuming in EASE layer.\n",
    "                                  This coefficient reduces number of ease parametrs by taking only\n",
    "                                  fraction of items sorted by popularity as input for model.\n",
    "                                  Note: This coef should be somewhere around coverage@100 achieved by full\n",
    "                                  size model.\n",
    "                                  For ML20M this coef should be between 0.4888 (coverage@100 for full model)\n",
    "                                  and 1.0\n",
    "                                  For Netflix this coef should be between 0.7055 (coverage@100 for full\n",
    "                                  model) and 1.0\n",
    "            \"\"\"\n",
    "            super(VASP.Model, self).__init__()\n",
    "\n",
    "            self.sampled_items = int(num_words * items_sampling)\n",
    "\n",
    "            assert self.sampled_items > 0\n",
    "            assert self.sampled_items <= num_words\n",
    "\n",
    "            self.s = self.sampled_items < num_words\n",
    "\n",
    "            # ************* ENCODER ***********************\n",
    "            self.encoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln5 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder6 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln6 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder7 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln7 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            # ************* SAMPLING **********************\n",
    "            self.dense_mean = tf.keras.layers.Dense(latent,\n",
    "                                                    # kernel_initializer=tf.keras.initializers.RandomNormal(0.0001),\n",
    "                                                    # activation='selu',\n",
    "                                                    # kernel_regularizer=tf.keras.regularizers.L2(l2=0.000001),\n",
    "                                                    # kernel_initializer=tf.keras.initializers.LecunNormal(),\n",
    "                                                    name=\"Mean\")\n",
    "            self.dense_log_var = tf.keras.layers.Dense(latent,\n",
    "                                                       # activation='sigmoid',\n",
    "                                                       name=\"log_var\")\n",
    "\n",
    "            self.sampling = Sampling(name='Sampler')\n",
    "\n",
    "            # ************* DECODER ***********************\n",
    "            self.decoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            self.decoder_resnet = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderR\")\n",
    "            self.decoder_latent = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderL\")\n",
    "\n",
    "            # ************* PARALLEL SHALLOW PATH *********\n",
    "\n",
    "            self.ease = tf.keras.layers.Dense(\n",
    "                self.sampled_items,\n",
    "                activation='sigmoid',\n",
    "                use_bias=False,\n",
    "                kernel_constraint=DiagonalToZero(),  # critical to prevent learning simple identity\n",
    "            )\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            sampling = self.s\n",
    "            if sampling:\n",
    "                sampled_x = x[:, :self.sampled_items]\n",
    "                non_sampled = x[:, self.sampled_items:] * 0.\n",
    "            else:\n",
    "                sampled_x = x\n",
    "\n",
    "            z_mean, z_log_var, z = self.encode(sampled_x)\n",
    "            if training:\n",
    "                d = self.decode(z)\n",
    "                # Add KL divergence regularization loss.\n",
    "                kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "                kl_loss = tf.reduce_mean(kl_loss)\n",
    "                kl_loss *= -0.5\n",
    "                self.add_loss(kl_loss)\n",
    "                self.add_metric(kl_loss, name=\"kl_div\")\n",
    "            else:\n",
    "                d = self.decode(z_mean)\n",
    "\n",
    "            if sampling:\n",
    "                d = tf.concat([d, non_sampled], axis=-1)\n",
    "\n",
    "            ease = self.ease(sampled_x)\n",
    "\n",
    "            if sampling:\n",
    "                ease = tf.concat([ease, non_sampled], axis=-1)\n",
    "\n",
    "            return d * ease\n",
    "\n",
    "        def decode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.dln1(tf.keras.activations.swish(self.decoder1(e0)))\n",
    "            e2 = self.dln2(tf.keras.activations.swish(self.decoder2(e1) + e1))\n",
    "            e3 = self.dln3(tf.keras.activations.swish(self.decoder3(e2) + e1 + e2))\n",
    "            e4 = self.dln4(tf.keras.activations.swish(self.decoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.dln5(tf.keras.activations.swish(self.decoder5(e4) + e1 + e2 + e3 + e4))\n",
    "\n",
    "            dr = self.decoder_resnet(e5)\n",
    "            dl = self.decoder_latent(x)\n",
    "\n",
    "            return dr * dl\n",
    "\n",
    "        def encode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.ln1(tf.keras.activations.swish(self.encoder1(e0)))\n",
    "            e2 = self.ln2(tf.keras.activations.swish(self.encoder2(e1) + e1))\n",
    "            e3 = self.ln3(tf.keras.activations.swish(self.encoder3(e2) + e1 + e2))\n",
    "            e4 = self.ln4(tf.keras.activations.swish(self.encoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.ln5(tf.keras.activations.swish(self.encoder5(e4) + e1 + e2 + e3 + e4))\n",
    "            e6 = self.ln6(tf.keras.activations.swish(self.encoder6(e5) + e1 + e2 + e3 + e4 + e5))\n",
    "            e7 = self.ln7(tf.keras.activations.swish(self.encoder7(e6) + e1 + e2 + e3 + e4 + e5 + e6))\n",
    "\n",
    "            z_mean = self.dense_mean(e7)\n",
    "            z_log_var = self.dense_log_var(e7)\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "            return z_mean, z_log_var, z\n",
    "\n",
    "    def create_model(self, latent=2048, hidden=4096, ease_items_sampling=1., summary=False):\n",
    "        self.model = VASP.Model(self.dataset.num_words, latent, hidden, ease_items_sampling)\n",
    "        self.model(self.split.train_gen[0][0])\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "        self.mc = MetricsCallback(self)\n",
    "\n",
    "    def compile_model(self, lr=0.00002, fl_alpha=0.25, fl_gamma=2.0):\n",
    "        \"\"\"\n",
    "        lr         learning rate of Nadam optimizer\n",
    "        fl_alpha   alpha parameter of focal crossentropy\n",
    "        fl_gamma   gamma parameter of focal crossentropy\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Nadam(lr),\n",
    "            loss=lambda x, y: tfa.losses.sigmoid_focal_crossentropy(x, y, alpha=fl_alpha, gamma=fl_gamma),\n",
    "            metrics=['mse', cosine_loss]\n",
    "        )\n",
    "\n",
    "    def train_model(self, epochs=150):\n",
    "        self.model.fit(\n",
    "            self.split.train_gen,\n",
    "            validation_data=self.split.validation_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=[self.mc]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efficient-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 0.6268243789672852\n",
      "Tokenizer trained for 20721 items.\n",
      "Creating 1 splits of 10000 samples each.\n",
      "Creating split nr. 1\n",
      "SplitGenerator init done in 4.550221681594849 secs.\n",
      "SplitGenerator init done in 0.38109278678894043 secs.\n",
      "SplitGenerator init done in 0.3804643154144287 secs.\n",
      "Creating evaluator\n",
      "Creating test split evaluator with leave_random_20_pct_out method.\n",
      "Creating validation split evaluator with leave_random_20_pct_out method.\n"
     ]
    }
   ],
   "source": [
    "dataset = Data(d='', pruning='u5')\n",
    "dataset.splits = []\n",
    "dataset.create_splits(1, 10000, shuffle=False, generators=False)\n",
    "dataset.split.train_users = pd.read_json(\"train_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.validation_users = pd.read_json(\"val_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.test_users = pd.read_json(\"test_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-surprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  25464832  \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "Mean (Dense)                 multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "log_var (Dense)              multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "Sampler (Sampling)           multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_8 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_9 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_11 (Laye multiple                  8192      \n",
      "_________________________________________________________________\n",
      "DecoderR (Dense)             multiple                  25466952  \n",
      "_________________________________________________________________\n",
      "DecoderL (Dense)             multiple                  12736584  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  38638656  \n",
      "=================================================================\n",
      "Total params: 295,392,464\n",
      "Trainable params: 295,392,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "================================================================================\n",
      "Train for 50 epochs with lr 0.00005\n",
      "Epoch 1/50\n",
      "112/112 [==============================] - 56s 468ms/step - loss: 21.3024 - mse: 0.0030 - cosine_loss: 0.8467 - kl_div: 1.0151 - val_loss: 27.2021 - val_mse: 0.0035 - val_cosine_loss: 0.7296 - val_kl_div: 0.0000e+00\n",
      "Model metrics:Recall@5=0.2306 Recall@20=0.2933 Recall@50=0.41 NCDG@100=0.323 Coverage@5=0.021 Coverage@20=0.0396 Coverage@50=0.0622 Coverage@100=0.0877 \n",
      "New best for NCDG@100\n",
      "New best for Recall@20\n",
      "New best for Recall@50\n",
      "Epoch 2/50\n",
      "112/112 [==============================] - 53s 469ms/step - loss: 16.6328 - mse: 0.0023 - cosine_loss: 0.8043 - kl_div: 0.5157 - val_loss: 25.6572 - val_mse: 0.0034 - val_cosine_loss: 0.7050 - val_kl_div: 0.0000e+00\n",
      "Model metrics:Recall@5=0.2565 Recall@20=0.3197 Recall@50=0.4426 NCDG@100=0.3521 Coverage@5=0.0335 Coverage@20=0.0593 Coverage@50=0.0869 Coverage@100=0.1158 \n",
      "New best for NCDG@100\n",
      "New best for Recall@20\n",
      "New best for Recall@50\n",
      "Epoch 3/50\n",
      "112/112 [==============================] - 52s 464ms/step - loss: 16.0210 - mse: 0.0023 - cosine_loss: 0.7901 - kl_div: 0.4044 - val_loss: 24.9851 - val_mse: 0.0034 - val_cosine_loss: 0.6937 - val_kl_div: 0.0000e+00\n",
      "Model metrics:Recall@5=0.2685 Recall@20=0.333 Recall@50=0.4591 NCDG@100=0.3668 Coverage@5=0.0372 Coverage@20=0.0669 Coverage@50=0.0985 Coverage@100=0.1311 \n",
      "New best for NCDG@100\n",
      "New best for Recall@20\n"
     ]
    }
   ],
   "source": [
    "m = VASP(dataset.split, name=\"VASP_ML20_1\")\n",
    "m.create_model(latent=2048, hidden=4096, ease_items_sampling=0.3)\n",
    "m.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for 50 epochs with lr 0.00005\")\n",
    "m.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(50)\n",
    "print(\"=\" * 80)\n",
    "print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(20)\n",
    "print(\"=\" * 80)\n",
    "print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold evaluation on the test set\n",
    "\n",
    "test_r20s = []\n",
    "test_r50s = []\n",
    "test_n100s = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    ev=Evaluator(m.split, method=str(fold)+'_20')\n",
    "    ev.update(m.model)\n",
    "\n",
    "    test_n100s.append(ev.get_ncdg(100))\n",
    "    test_r20s.append(ev.get_recall(20))\n",
    "    test_r50s.append(ev.get_recall(50))\n",
    "\n",
    "print(\"TEST SET (MEAN)\")\n",
    "print(\"5-fold mean NCDG@100\", round(sum(test_n100s) / len(test_n100s),3))\n",
    "print(\"5-fold mean Recall@20\", round(sum(test_r20s) / len(test_r20s),3))\n",
    "print(\"5-fold mean Recall@50\", round(sum(test_r50s) / len(test_r50s),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-contamination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-forth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
