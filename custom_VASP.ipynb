{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metallic-vietnamese",
   "metadata": {},
   "source": [
    "First we need to import utils package and tensorflow with addons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 10:40:43.728785: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-02 10:40:44.330840: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-05-02 10:40:44.330929: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-05-02 10:40:44.330935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime  \n",
    "\n",
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "tf.config.experimental.set_memory_growth\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa60cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c36e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/vasp/Datasets/ml-20m/preprocessed_vasp/',\n",
       " 'VASP_ml-20m_02-05-2023_10-40-45')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_name = 'ml-20m'\n",
    "# ml-20m: len(self.pr_val) = 13668\n",
    "# divisors: {1,2,3,4,6,12,17,34,51,67,68,102,134,201,204,268,402,804,1139,2278,3417,4556,6834,13668}\n",
    "chunk_val = 4556\n",
    "# ml-20m: len(self.pr_test) = 13667\n",
    "# divisors: {1,79,173,13667}\n",
    "chunk_test = 173\n",
    "\n",
    "# ds_name = 'netflix'\n",
    "# # netflix: len(self.pr_val) = 46344\n",
    "# # divisors: {1,2,3,4,6,8,12,24,1931,3862,5793,7724,11586,15448,23172,46344}\n",
    "# chunk_val= 1931\n",
    "# # netflix: len(self.pr_test) = 46343\n",
    "# # divisors: {1,11,121,383,4213,46343}\n",
    "# chunk_test= 383\n",
    "\n",
    "# ds_name = 'steam-200k'\n",
    "# # steam: len(self.pr) = 165\n",
    "# # divisors: {1,3,5,11,15,33,55,165}\n",
    "# chunk_val = 165\n",
    "# # steam: len(self.pr) = 163\n",
    "# # divisors: {1,163}\n",
    "# chunk_test = 163\n",
    "\n",
    "data_path = os.path.join('/home/ubuntu/vasp/Datasets/', ds_name, 'preprocessed_vasp/')\n",
    "\n",
    "str_date_time = datetime.fromtimestamp(datetime.now().timestamp()).strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "model_name = str('VASP_'+ds_name+'_'+str_date_time)\n",
    "\n",
    "data_path, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-connecticut",
   "metadata": {},
   "source": [
    "Next, we define our model. Note that reparametrization trick is done manually - it can be done with tensorflow probability package, which is doing that by itself automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalToZero(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        \"\"\"Set diagonal to zero\"\"\"\n",
    "        q = tf.linalg.set_diag(w, tf.zeros(w.shape[0:-1]), name=None)\n",
    "        return q\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a basket.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VASP(Model):\n",
    "    class Model(tf.keras.Model):\n",
    "        def __init__(self, num_words, latent=1024, hidden=1024, items_sampling=1.):\n",
    "            \"\"\"\n",
    "            num_words             nr of items in dataset (size of tokenizer)\n",
    "            latent                size of latent space\n",
    "            hidden                size of hidden layers\n",
    "            items_sampling        Large items datatsets can be very gpu memory consuming in EASE layer.\n",
    "                                  This coefficient reduces number of ease parametrs by taking only\n",
    "                                  fraction of items sorted by popularity as input for model.\n",
    "                                  Note: This coef should be somewhere around coverage@100 achieved by full\n",
    "                                  size model.\n",
    "                                  For ML20M this coef should be between 0.4888 (coverage@100 for full model)\n",
    "                                  and 1.0\n",
    "                                  For Netflix this coef should be between 0.7055 (coverage@100 for full\n",
    "                                  model) and 1.0\n",
    "            \"\"\"\n",
    "            super(VASP.Model, self).__init__()\n",
    "\n",
    "            self.sampled_items = int(num_words * items_sampling)\n",
    "\n",
    "            assert self.sampled_items > 0\n",
    "            assert self.sampled_items <= num_words\n",
    "\n",
    "            self.s = self.sampled_items < num_words\n",
    "\n",
    "            # ************* ENCODER ***********************\n",
    "            self.encoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln5 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder6 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln6 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder7 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln7 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            # ************* SAMPLING **********************\n",
    "            self.dense_mean = tf.keras.layers.Dense(latent,\n",
    "                                                    name=\"Mean\")\n",
    "            self.dense_log_var = tf.keras.layers.Dense(latent,\n",
    "                                                       name=\"log_var\")\n",
    "\n",
    "            self.sampling = Sampling(name='Sampler')\n",
    "\n",
    "            # ************* DECODER ***********************\n",
    "            self.decoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            self.decoder_resnet = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderR\")\n",
    "            self.decoder_latent = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderL\")\n",
    "\n",
    "            # ************* PARALLEL SHALLOW PATH *********\n",
    "\n",
    "            self.ease = tf.keras.layers.Dense(\n",
    "                self.sampled_items,\n",
    "                activation='sigmoid',\n",
    "                use_bias=False,\n",
    "                kernel_constraint=DiagonalToZero(),  # critical to prevent learning simple identity\n",
    "            )\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            sampling = self.s\n",
    "            if sampling:\n",
    "                sampled_x = x[:, :self.sampled_items]\n",
    "                non_sampled = x[:, self.sampled_items:] * 0.\n",
    "            else:\n",
    "                sampled_x = x\n",
    "\n",
    "            z_mean, z_log_var, z = self.encode(sampled_x)\n",
    "            if training:\n",
    "                d = self.decode(z)\n",
    "                # Add KL divergence regularization loss.\n",
    "                kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "                kl_loss = tf.reduce_mean(kl_loss)\n",
    "                kl_loss *= -0.5\n",
    "                self.add_loss(kl_loss)\n",
    "                self.add_metric(kl_loss, name=\"kl_div\")\n",
    "            else:\n",
    "                d = self.decode(z_mean)\n",
    "\n",
    "            if sampling:\n",
    "                d = tf.concat([d, non_sampled], axis=-1)\n",
    "\n",
    "            ease = self.ease(sampled_x)\n",
    "\n",
    "            if sampling:\n",
    "                ease = tf.concat([ease, non_sampled], axis=-1)\n",
    "\n",
    "            return d * ease\n",
    "\n",
    "        def decode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.dln1(tf.keras.activations.swish(self.decoder1(e0)))\n",
    "            e2 = self.dln2(tf.keras.activations.swish(self.decoder2(e1) + e1))\n",
    "            e3 = self.dln3(tf.keras.activations.swish(self.decoder3(e2) + e1 + e2))\n",
    "            e4 = self.dln4(tf.keras.activations.swish(self.decoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.dln5(tf.keras.activations.swish(self.decoder5(e4) + e1 + e2 + e3 + e4))\n",
    "\n",
    "            dr = self.decoder_resnet(e5)\n",
    "            dl = self.decoder_latent(x)\n",
    "\n",
    "            return dr * dl\n",
    "\n",
    "        def encode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.ln1(tf.keras.activations.swish(self.encoder1(e0)))\n",
    "            e2 = self.ln2(tf.keras.activations.swish(self.encoder2(e1) + e1))\n",
    "            e3 = self.ln3(tf.keras.activations.swish(self.encoder3(e2) + e1 + e2))\n",
    "            e4 = self.ln4(tf.keras.activations.swish(self.encoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.ln5(tf.keras.activations.swish(self.encoder5(e4) + e1 + e2 + e3 + e4))\n",
    "            e6 = self.ln6(tf.keras.activations.swish(self.encoder6(e5) + e1 + e2 + e3 + e4 + e5))\n",
    "            e7 = self.ln7(tf.keras.activations.swish(self.encoder7(e6) + e1 + e2 + e3 + e4 + e5 + e6))\n",
    "\n",
    "            z_mean = self.dense_mean(e7)\n",
    "            z_log_var = self.dense_log_var(e7)\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "            return z_mean, z_log_var, z\n",
    "\n",
    "    def create_model(self, latent=2048, hidden=4096, ease_items_sampling=1., summary=False):\n",
    "        self.model = VASP.Model(self.dataset.num_words, latent, hidden, ease_items_sampling)\n",
    "        self.model(self.split.train_gen[0][0])\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "        self.mc = MetricsCallback(self)\n",
    "\n",
    "    def compile_model(self, lr=0.00002, fl_alpha=0.25, fl_gamma=2.0):\n",
    "        \"\"\"\n",
    "        lr         learning rate of Nadam optimizer\n",
    "        fl_alpha   alpha parameter of focal crossentropy\n",
    "        fl_gamma   gamma parameter of focal crossentropy\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Nadam(lr),\n",
    "            loss=lambda x, y: tfa.losses.sigmoid_focal_crossentropy(x, y, alpha=fl_alpha, gamma=fl_gamma),\n",
    "            metrics=['mse', cosine_loss],\n",
    "            run_eagerly=True\n",
    "        )\n",
    "\n",
    "    def train_model(self, epochs=150, batch_size=64):\n",
    "        self.model.fit(\n",
    "            self.split.train_gen,\n",
    "            validation_data=self.split.validation_gen,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[self.mc]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-ambassador",
   "metadata": {},
   "source": [
    "Now, we can load previously preprocessed dataset. We also load pre-defined train/test/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "medium-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 0.7526662349700928\n",
      "Tokenizer trained for 19908 items.\n",
      "Creating 1 splits of 100 samples each.\n",
      "Creating split nr. 1\n",
      "SplitGenerator init done in 4.209919452667236 secs.\n",
      "SplitGenerator init done in 0.5124008655548096 secs.\n",
      "SplitGenerator init done in 0.5164525508880615 secs.\n",
      "Creating evaluator\n",
      "Creating test split evaluator with leave_random_20_pct_out method.\n",
      "tpx_set: 13667\n",
      "tpx_set: 17\n",
      "type tpx_set: <class 'list'>\n",
      "iv: 13667\n",
      "iv: 19908\n",
      "type iv: <class 'list'>\n",
      "Creating validation split evaluator with leave_random_20_pct_out method.\n",
      "tpx_set: 13668\n",
      "tpx_set: 35\n",
      "type tpx_set: <class 'list'>\n",
      "iv: 13668\n",
      "iv: 19908\n",
      "type iv: <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.SplitGenerator at 0x7f9284678b50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Data(d=data_path, pruning='u5')\n",
    "dataset.splits = []\n",
    "\n",
    "dataset.create_splits(1, 100, shuffle=False, n_fold=False, generators=False, batch_size=64, chunk=chunk_val)\n",
    "dataset.split.train_users = pd.read_json(os.path.join(data_path, \"train_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.validation_users = pd.read_json(os.path.join(data_path, \"val_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.test_users = pd.read_json(os.path.join(data_path, \"test_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.generators()\n",
    "\n",
    "dataset.split.train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3421c241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.split.train_gen.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-handling",
   "metadata": {},
   "source": [
    "Now we can create an instance of our model and train it.\n",
    "\n",
    "For validation during training we sample 80% of the user's interactions randomly as input for the model, and then we measure Recall@20, Recall@50 and NDCG@100 for predicted interactions against the remaining 20% of the user's interactions.\n",
    "\n",
    "This method can give different results for different seeds, but since it is used for validation during training only, it's good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intellectual-mumbai",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Train for 1 epochs with lr 0.00005\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fefddbefd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fefddbefd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "143/143 [==============================] - 5s 32ms/step\n",
      "143/143 [==============================] - 5s 32ms/step\n",
      "143/143 [==============================] - 5s 32ms/step\n",
      "iv: 13668\n",
      "pr: 13668\n",
      "ivx: 13668\n",
      "range: range(0, 3)\n",
      "self.chunk: 4556\n",
      "<class 'list'>\n",
      "100\n",
      "13668\n",
      "100\n",
      "100\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.14812344 ... 0.         0.         0.        ]\n",
      " [0.         0.17495601 0.22018221 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.47919962 ... 0.         0.         0.        ]\n",
      " [0.         0.28088802 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.20303221 0.         ... 0.         0.         0.        ]]\n",
      "[{'889', '214', '597', '1004', '757', '65', '13', '231', '258', '44', '1643', '1056', '8', '208', '3', '195', '364', '292', '376', '7094', '11', '365'}, {'1211', '2092', '1085', '1992', '1654', '584', '314', '200', '205', '495', '44', '633', '51', '2263'}, {'5348', '2169', '3743', '6657', '10', '5008', '218', '2265', '7379', '216', '33', '153', '142', '2421', '1026', '1393', '78', '1624', '98', '3500', '2792', '352', '551', '1003'}, {'465', '301', '1609', '488', '95', '696', '96', '170', '492', '383', '64', '637'}, {'1219', '49'}, {'56', '1067', '2039'}, {'2040', '83', '1231', '2042', '1752', '82', '170', '595', '374', '2009', '3521', '1430', '1422', '7104', '93', '65', '1252', '1336', '1507', '156', '999', '4833', '205', '2002', '57', '1634', '44', '87', '1055', '89', '2928', '1331', '1622', '62', '1280', '71', '594', '216', '53', '1776', '630', '1020', '966', '638', '633', '454', '607', '1146', '294', '1251', '301', '1091', '123', '81', '359', '6', '1584', '1154', '1624', '1086', '2039'}, {'14', '77', '191', '218', '983', '978', '377', '340', '673', '205', '374', '1177', '129', '454', '1653', '357', '13'}, {'11', '176', '13'}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vasp/custom_VASP.ipynb Zelle 11\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain for \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(n_epochs)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m epochs with lr 0.00005\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m m\u001b[39m.\u001b[39mcompile_model(lr\u001b[39m=\u001b[39m\u001b[39m0.00005\u001b[39m, fl_alpha\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m, fl_gamma\u001b[39m=\u001b[39m\u001b[39m2.0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m m\u001b[39m.\u001b[39;49mtrain_model(n_epochs)\n",
      "\u001b[1;32m/home/ubuntu/vasp/custom_VASP.ipynb Zelle 11\u001b[0m in \u001b[0;36mVASP.train_model\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(\u001b[39mself\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=175'>176</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit\u001b[39m.\u001b[39;49mtrain_gen,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=176'>177</a>\u001b[0m         validation_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit\u001b[39m.\u001b[39;49mvalidation_gen,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=177'>178</a>\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=178'>179</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=179'>180</a>\u001b[0m         callbacks\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmc]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=180'>181</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/vasp/utils.py:768\u001b[0m, in \u001b[0;36mMetricsCallback.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_metrics[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch][x] \u001b[39m=\u001b[39m logs[x]\n\u001b[1;32m    767\u001b[0m \u001b[39m# add custom metrics\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrsmodel\u001b[39m.\u001b[39;49mevaluate_model()\n\u001b[1;32m    769\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrsmodel\u001b[39m.\u001b[39mprint_metrics()\n\u001b[1;32m    770\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrsmodel\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/vasp/utils.py:710\u001b[0m, in \u001b[0;36mModel.evaluate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    709\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 710\u001b[0m     x[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m x[\u001b[39m'\u001b[39;49m\u001b[39mmethod\u001b[39;49m\u001b[39m'\u001b[39;49m](x[\u001b[39m'\u001b[39;49m\u001b[39mk\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/vasp/utils.py:574\u001b[0m, in \u001b[0;36mEvaluator.custom_ndcg_at_rank_k\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    571\u001b[0m prediction_ids \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mmaster_data\u001b[39m.\u001b[39mtoki\u001b[39m.\u001b[39mindex_word[b] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m a] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m idx_topk]\n\u001b[1;32m    572\u001b[0m target_ids \u001b[39m=\u001b[39m prediction_ids \u001b[39m#self.ivx_set\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m dcg \u001b[39m=\u001b[39m custom_calculate_dcg(prediction_ids[:k], target_ids)\n\u001b[1;32m    575\u001b[0m idcg \u001b[39m=\u001b[39m custom_calculate_dcg(target_ids[:k], target_ids)\n\u001b[1;32m    576\u001b[0m ndcg \u001b[39m=\u001b[39m dcg \u001b[39m/\u001b[39m idcg \u001b[39mif\u001b[39;00m idcg \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/vasp/utils.py:547\u001b[0m, in \u001b[0;36mEvaluator.custom_ndcg_at_rank_k.<locals>.custom_calculate_dcg\u001b[0;34m(prediction_ids, target_ids)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mppp[:\u001b[39m9\u001b[39m])\n\u001b[1;32m    546\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtpx_set[:\u001b[39m9\u001b[39m])\n\u001b[0;32m--> 547\u001b[0m prediction_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mdict\u001b[39;49m\u001b[39m.\u001b[39;49mfromkeys(prediction_ids))\n\u001b[1;32m    549\u001b[0m target_ids \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(target_ids)\n\u001b[1;32m    551\u001b[0m dcg \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "m = VASP(dataset.split, name=model_name)\n",
    "m.create_model(latent=2048, hidden=4096, ease_items_sampling=0.33)\n",
    "n_epochs = 1\n",
    "#m.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for \"+str(n_epochs)+\" epochs with lr 0.00005\")\n",
    "m.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(n_epochs)\n",
    "#print(\"=\" * 80)\n",
    "#print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "#m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "#m.train_model(20)\n",
    "#print(\"=\" * 80)\n",
    "#print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "#m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "#m.train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "m = tf.keras.models.load_model('/home/ubuntu/vasp/VASP_netflix_27-04-2023_20-25-43_best_ncdg_100_model/VASP_netflix_27-04-2023_20-25-43')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-fantasy",
   "metadata": {},
   "source": [
    "Dataframe with the details of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mc.get_history_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-collins",
   "metadata": {},
   "source": [
    "And the details of the training as a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mc.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-payroll",
   "metadata": {},
   "source": [
    "For final evaluation we do 5-fold validation on the user's interaction history. We think that this is more objective measure than only hide sampled 20% of the user's interactions randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold evaluation on the test set\n",
    "\n",
    "test_r20s = []\n",
    "test_r50s = []\n",
    "test_n100s = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    ev=Evaluator(m.split, method=str(fold)+'_20', chunk=chunk_test)\n",
    "    ev.update(m.model)\n",
    "\n",
    "    test_n100s.append(ev.get_ncdg(100))\n",
    "    test_r20s.append(ev.get_recall(20))\n",
    "    test_r50s.append(ev.get_recall(50))\n",
    "\n",
    "print(\"TEST SET (MEAN)\")\n",
    "print(\"5-fold mean NCDG@100\", round(sum(test_n100s) / len(test_n100s),3))\n",
    "print(\"5-fold mean Recall@20\", round(sum(test_r20s) / len(test_r20s),3))\n",
    "print(\"5-fold mean Recall@50\", round(sum(test_r50s) / len(test_r50s),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
