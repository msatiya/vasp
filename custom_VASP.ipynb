{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metallic-vietnamese",
   "metadata": {},
   "source": [
    "First we need to import utils package and tensorflow with addons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-28 10:34:50.737449: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-28 10:34:51.339070: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-04-28 10:34:51.339158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-04-28 10:34:51.339164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime  \n",
    "\n",
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "tf.config.experimental.set_memory_growth\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa60cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c36e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/vasp/Datasets/ml-20m/preprocessed_vasp/',\n",
       " 'VASP_ml-20m_28-04-2023_10-34-53')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_name = 'ml-20m'\n",
    "# ml-20m: len(self.pr_val) = 13668\n",
    "# divisors: {1,2,3,4,6,12,17,34,51,67,68,102,134,201,204,268,402,804,1139,2278,3417,4556,6834,13668}\n",
    "chunk_val = 2278\n",
    "# ml-20m: len(self.pr_test) = 13667\n",
    "# divisors: {1,79,173,13667}\n",
    "chunk_test = 173\n",
    "\n",
    "# ds_name = 'netflix'\n",
    "# # netflix: len(self.pr_val) = 46344\n",
    "# # divisors: {1,2,3,4,6,8,12,24,1931,3862,5793,7724,11586,15448,23172,46344}\n",
    "# chunk_val= 1931\n",
    "# # netflix: len(self.pr_test) = 46343\n",
    "# # divisors: {1,11,121,383,4213,46343}\n",
    "# chunk_test= 383\n",
    "\n",
    "# ds_name = 'steam-200k'\n",
    "# # steam: len(self.pr) = 165\n",
    "# # divisors: {1,3,5,11,15,33,55,165}\n",
    "# chunk_val = 165\n",
    "# # steam: len(self.pr) = 163\n",
    "# # divisors: {1,163}\n",
    "# chunk_test = 163\n",
    "\n",
    "data_path = os.path.join('/home/ubuntu/vasp/Datasets/', ds_name, 'preprocessed_vasp/')\n",
    "\n",
    "str_date_time = datetime.fromtimestamp(datetime.now().timestamp()).strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "model_name = str('VASP_'+ds_name+'_'+str_date_time)\n",
    "\n",
    "data_path, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-connecticut",
   "metadata": {},
   "source": [
    "Next, we define our model. Note that reparametrization trick is done manually - it can be done with tensorflow probability package, which is doing that by itself automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalToZero(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        \"\"\"Set diagonal to zero\"\"\"\n",
    "        q = tf.linalg.set_diag(w, tf.zeros(w.shape[0:-1]), name=None)\n",
    "        return q\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a basket.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VASP(Model):\n",
    "    class Model(tf.keras.Model):\n",
    "        def __init__(self, num_words, latent=1024, hidden=1024, items_sampling=1.):\n",
    "            \"\"\"\n",
    "            num_words             nr of items in dataset (size of tokenizer)\n",
    "            latent                size of latent space\n",
    "            hidden                size of hidden layers\n",
    "            items_sampling        Large items datatsets can be very gpu memory consuming in EASE layer.\n",
    "                                  This coefficient reduces number of ease parametrs by taking only\n",
    "                                  fraction of items sorted by popularity as input for model.\n",
    "                                  Note: This coef should be somewhere around coverage@100 achieved by full\n",
    "                                  size model.\n",
    "                                  For ML20M this coef should be between 0.4888 (coverage@100 for full model)\n",
    "                                  and 1.0\n",
    "                                  For Netflix this coef should be between 0.7055 (coverage@100 for full\n",
    "                                  model) and 1.0\n",
    "            \"\"\"\n",
    "            super(VASP.Model, self).__init__()\n",
    "\n",
    "            self.sampled_items = int(num_words * items_sampling)\n",
    "\n",
    "            assert self.sampled_items > 0\n",
    "            assert self.sampled_items <= num_words\n",
    "\n",
    "            self.s = self.sampled_items < num_words\n",
    "\n",
    "            # ************* ENCODER ***********************\n",
    "            self.encoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln5 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder6 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln6 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder7 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln7 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            # ************* SAMPLING **********************\n",
    "            self.dense_mean = tf.keras.layers.Dense(latent,\n",
    "                                                    name=\"Mean\")\n",
    "            self.dense_log_var = tf.keras.layers.Dense(latent,\n",
    "                                                       name=\"log_var\")\n",
    "\n",
    "            self.sampling = Sampling(name='Sampler')\n",
    "\n",
    "            # ************* DECODER ***********************\n",
    "            self.decoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            self.decoder_resnet = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderR\")\n",
    "            self.decoder_latent = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderL\")\n",
    "\n",
    "            # ************* PARALLEL SHALLOW PATH *********\n",
    "\n",
    "            self.ease = tf.keras.layers.Dense(\n",
    "                self.sampled_items,\n",
    "                activation='sigmoid',\n",
    "                use_bias=False,\n",
    "                kernel_constraint=DiagonalToZero(),  # critical to prevent learning simple identity\n",
    "            )\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            sampling = self.s\n",
    "            if sampling:\n",
    "                sampled_x = x[:, :self.sampled_items]\n",
    "                non_sampled = x[:, self.sampled_items:] * 0.\n",
    "            else:\n",
    "                sampled_x = x\n",
    "\n",
    "            z_mean, z_log_var, z = self.encode(sampled_x)\n",
    "            if training:\n",
    "                d = self.decode(z)\n",
    "                # Add KL divergence regularization loss.\n",
    "                kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "                kl_loss = tf.reduce_mean(kl_loss)\n",
    "                kl_loss *= -0.5\n",
    "                self.add_loss(kl_loss)\n",
    "                self.add_metric(kl_loss, name=\"kl_div\")\n",
    "            else:\n",
    "                d = self.decode(z_mean)\n",
    "\n",
    "            if sampling:\n",
    "                d = tf.concat([d, non_sampled], axis=-1)\n",
    "\n",
    "            ease = self.ease(sampled_x)\n",
    "\n",
    "            if sampling:\n",
    "                ease = tf.concat([ease, non_sampled], axis=-1)\n",
    "\n",
    "            return d * ease\n",
    "\n",
    "        def decode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.dln1(tf.keras.activations.swish(self.decoder1(e0)))\n",
    "            e2 = self.dln2(tf.keras.activations.swish(self.decoder2(e1) + e1))\n",
    "            e3 = self.dln3(tf.keras.activations.swish(self.decoder3(e2) + e1 + e2))\n",
    "            e4 = self.dln4(tf.keras.activations.swish(self.decoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.dln5(tf.keras.activations.swish(self.decoder5(e4) + e1 + e2 + e3 + e4))\n",
    "\n",
    "            dr = self.decoder_resnet(e5)\n",
    "            dl = self.decoder_latent(x)\n",
    "\n",
    "            return dr * dl\n",
    "\n",
    "        def encode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.ln1(tf.keras.activations.swish(self.encoder1(e0)))\n",
    "            e2 = self.ln2(tf.keras.activations.swish(self.encoder2(e1) + e1))\n",
    "            e3 = self.ln3(tf.keras.activations.swish(self.encoder3(e2) + e1 + e2))\n",
    "            e4 = self.ln4(tf.keras.activations.swish(self.encoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.ln5(tf.keras.activations.swish(self.encoder5(e4) + e1 + e2 + e3 + e4))\n",
    "            e6 = self.ln6(tf.keras.activations.swish(self.encoder6(e5) + e1 + e2 + e3 + e4 + e5))\n",
    "            e7 = self.ln7(tf.keras.activations.swish(self.encoder7(e6) + e1 + e2 + e3 + e4 + e5 + e6))\n",
    "\n",
    "            z_mean = self.dense_mean(e7)\n",
    "            z_log_var = self.dense_log_var(e7)\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "            return z_mean, z_log_var, z\n",
    "\n",
    "    def create_model(self, latent=2048, hidden=4096, ease_items_sampling=1., summary=False):\n",
    "        self.model = VASP.Model(self.dataset.num_words, latent, hidden, ease_items_sampling)\n",
    "        self.model(self.split.train_gen[0][0])\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "        self.mc = MetricsCallback(self)\n",
    "\n",
    "    def compile_model(self, lr=0.00002, fl_alpha=0.25, fl_gamma=2.0):\n",
    "        \"\"\"\n",
    "        lr         learning rate of Nadam optimizer\n",
    "        fl_alpha   alpha parameter of focal crossentropy\n",
    "        fl_gamma   gamma parameter of focal crossentropy\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Nadam(lr),\n",
    "            loss=lambda x, y: tfa.losses.sigmoid_focal_crossentropy(x, y, alpha=fl_alpha, gamma=fl_gamma),\n",
    "            metrics=['mse', cosine_loss],\n",
    "            run_eagerly=True\n",
    "        )\n",
    "\n",
    "    def train_model(self, epochs=150, batch_size=64):\n",
    "        self.model.fit(\n",
    "            self.split.train_gen,\n",
    "            validation_data=self.split.validation_gen,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[self.mc]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-ambassador",
   "metadata": {},
   "source": [
    "Now, we can load previously preprocessed dataset. We also load pre-defined train/test/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "medium-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 0.7573318481445312\n",
      "Tokenizer trained for 19908 items.\n",
      "Creating 1 splits of 100 samples each.\n",
      "Creating split nr. 1\n",
      "SplitGenerator init done in 4.235065937042236 secs.\n",
      "SplitGenerator init done in 0.5163004398345947 secs.\n",
      "SplitGenerator init done in 0.5130972862243652 secs.\n",
      "Creating evaluator\n",
      "Creating test split evaluator with leave_random_20_pct_out method.\n",
      "tpx_set: 13667\n",
      "ivx_set: 13667\n",
      "iv: 13667\n",
      "Creating validation split evaluator with leave_random_20_pct_out method.\n",
      "tpx_set: 13668\n",
      "ivx_set: 13668\n",
      "iv: 13668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils.SplitGenerator at 0x7f7cb41710d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Data(d=data_path, pruning='u5')\n",
    "dataset.splits = []\n",
    "\n",
    "dataset.create_splits(1, 100, shuffle=False, n_fold=False, generators=False, batch_size=64, chunk=chunk_val)\n",
    "dataset.split.train_users = pd.read_json(os.path.join(data_path, \"train_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.validation_users = pd.read_json(os.path.join(data_path, \"val_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.test_users = pd.read_json(os.path.join(data_path, \"test_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.generators()\n",
    "\n",
    "dataset.split.train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3421c241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.split.train_gen.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-handling",
   "metadata": {},
   "source": [
    "Now we can create an instance of our model and train it.\n",
    "\n",
    "For validation during training we sample 80% of the user's interactions randomly as input for the model, and then we measure Recall@20, Recall@50 and NDCG@100 for predicted interactions against the remaining 20% of the user's interactions.\n",
    "\n",
    "This method can give different results for different seeds, but since it is used for validation during training only, it's good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intellectual-mumbai",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Train for 5 epochs with lr 0.00005\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f7c35c2db80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f7c35c2db80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "105/105 [==============================] - ETA: 0s - loss: 31.9679 - mse: 0.0046 - cosine_loss: 0.7323 - kl_div: 1.2125"
     ]
    }
   ],
   "source": [
    "m = VASP(dataset.split, name=model_name)\n",
    "m.create_model(latent=2048, hidden=4096, ease_items_sampling=0.33)\n",
    "n_epochs = 5\n",
    "#m.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for \"+str(n_epochs)+\" epochs with lr 0.00005\")\n",
    "m.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(n_epochs)\n",
    "#print(\"=\" * 80)\n",
    "#print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "#m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "#m.train_model(20)\n",
    "#print(\"=\" * 80)\n",
    "#print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "#m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "#m.train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "m = tf.keras.models.load_model('/home/ubuntu/vasp/VASP_netflix_27-04-2023_20-25-43_best_ncdg_100_model/VASP_netflix_27-04-2023_20-25-43')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-fantasy",
   "metadata": {},
   "source": [
    "Dataframe with the details of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mc.get_history_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-collins",
   "metadata": {},
   "source": [
    "And the details of the training as a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mc.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-payroll",
   "metadata": {},
   "source": [
    "For final evaluation we do 5-fold validation on the user's interaction history. We think that this is more objective measure than only hide sampled 20% of the user's interactions randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold evaluation on the test set\n",
    "\n",
    "test_r20s = []\n",
    "test_r50s = []\n",
    "test_n100s = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    ev=Evaluator(m.split, method=str(fold)+'_20', chunk=chunk_test)\n",
    "    ev.update(m.model)\n",
    "\n",
    "    test_n100s.append(ev.get_ncdg(100))\n",
    "    test_r20s.append(ev.get_recall(20))\n",
    "    test_r50s.append(ev.get_recall(50))\n",
    "\n",
    "print(\"TEST SET (MEAN)\")\n",
    "print(\"5-fold mean NCDG@100\", round(sum(test_n100s) / len(test_n100s),3))\n",
    "print(\"5-fold mean Recall@20\", round(sum(test_r20s) / len(test_r20s),3))\n",
    "print(\"5-fold mean Recall@50\", round(sum(test_r50s) / len(test_r50s),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
