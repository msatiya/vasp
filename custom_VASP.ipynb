{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metallic-vietnamese",
   "metadata": {},
   "source": [
    "First we need to import utils package and tensorflow with addons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 19:14:26.749022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-25 19:14:27.351321: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-04-25 19:14:27.351410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-04-25 19:14:27.351415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ubuntu/.local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime  \n",
    "\n",
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa60cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c36e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/ubuntu/vasp/Datasets/steam-200k/preprocessed_vasp/',\n",
       " 'VASP_steam-200k_25-04-2023_19-14-28')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds_name = 'ml-20m'\n",
    "# ds_name = 'netflix'\n",
    "ds_name = 'steam-200k'\n",
    "\n",
    "data_path = os.path.join('/home/ubuntu/vasp/Datasets/', ds_name, 'preprocessed_vasp/')\n",
    "\n",
    "str_date_time = datetime.fromtimestamp(datetime.now().timestamp()).strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "model_name = str('VASP_'+ds_name+'_'+str_date_time)\n",
    "\n",
    "data_path, model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-connecticut",
   "metadata": {},
   "source": [
    "Next, we define our model. Note that reparametrization trick is done manually - it can be done with tensorflow probability package, which is doing that by itself automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalToZero(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        \"\"\"Set diagonal to zero\"\"\"\n",
    "        q = tf.linalg.set_diag(w, tf.zeros(w.shape[0:-1]), name=None)\n",
    "        return q\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a basket.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VASP(Model):\n",
    "    class Model(tf.keras.Model):\n",
    "        def __init__(self, num_words, latent=1024, hidden=1024, items_sampling=1.):\n",
    "            \"\"\"\n",
    "            num_words             nr of items in dataset (size of tokenizer)\n",
    "            latent                size of latent space\n",
    "            hidden                size of hidden layers\n",
    "            items_sampling        Large items datatsets can be very gpu memory consuming in EASE layer.\n",
    "                                  This coefficient reduces number of ease parametrs by taking only\n",
    "                                  fraction of items sorted by popularity as input for model.\n",
    "                                  Note: This coef should be somewhere around coverage@100 achieved by full\n",
    "                                  size model.\n",
    "                                  For ML20M this coef should be between 0.4888 (coverage@100 for full model)\n",
    "                                  and 1.0\n",
    "                                  For Netflix this coef should be between 0.7055 (coverage@100 for full\n",
    "                                  model) and 1.0\n",
    "            \"\"\"\n",
    "            super(VASP.Model, self).__init__()\n",
    "\n",
    "            self.sampled_items = int(num_words * items_sampling)\n",
    "\n",
    "            assert self.sampled_items > 0\n",
    "            assert self.sampled_items <= num_words\n",
    "\n",
    "            self.s = self.sampled_items < num_words\n",
    "\n",
    "            # ************* ENCODER ***********************\n",
    "            self.encoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln5 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder6 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln6 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder7 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln7 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            # ************* SAMPLING **********************\n",
    "            self.dense_mean = tf.keras.layers.Dense(latent,\n",
    "                                                    name=\"Mean\")\n",
    "            self.dense_log_var = tf.keras.layers.Dense(latent,\n",
    "                                                       name=\"log_var\")\n",
    "\n",
    "            self.sampling = Sampling(name='Sampler')\n",
    "\n",
    "            # ************* DECODER ***********************\n",
    "            self.decoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            self.decoder_resnet = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderR\")\n",
    "            self.decoder_latent = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderL\")\n",
    "\n",
    "            # ************* PARALLEL SHALLOW PATH *********\n",
    "\n",
    "            self.ease = tf.keras.layers.Dense(\n",
    "                self.sampled_items,\n",
    "                activation='sigmoid',\n",
    "                use_bias=False,\n",
    "                kernel_constraint=DiagonalToZero(),  # critical to prevent learning simple identity\n",
    "            )\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            sampling = self.s\n",
    "            if sampling:\n",
    "                sampled_x = x[:, :self.sampled_items]\n",
    "                non_sampled = x[:, self.sampled_items:] * 0.\n",
    "            else:\n",
    "                sampled_x = x\n",
    "\n",
    "            z_mean, z_log_var, z = self.encode(sampled_x)\n",
    "            if training:\n",
    "                d = self.decode(z)\n",
    "                # Add KL divergence regularization loss.\n",
    "                kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "                kl_loss = tf.reduce_mean(kl_loss)\n",
    "                kl_loss *= -0.5\n",
    "                self.add_loss(kl_loss)\n",
    "                self.add_metric(kl_loss, name=\"kl_div\")\n",
    "            else:\n",
    "                d = self.decode(z_mean)\n",
    "\n",
    "            if sampling:\n",
    "                d = tf.concat([d, non_sampled], axis=-1)\n",
    "\n",
    "            ease = self.ease(sampled_x)\n",
    "\n",
    "            if sampling:\n",
    "                ease = tf.concat([ease, non_sampled], axis=-1)\n",
    "\n",
    "            return d * ease\n",
    "\n",
    "        def decode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.dln1(tf.keras.activations.swish(self.decoder1(e0)))\n",
    "            e2 = self.dln2(tf.keras.activations.swish(self.decoder2(e1) + e1))\n",
    "            e3 = self.dln3(tf.keras.activations.swish(self.decoder3(e2) + e1 + e2))\n",
    "            e4 = self.dln4(tf.keras.activations.swish(self.decoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.dln5(tf.keras.activations.swish(self.decoder5(e4) + e1 + e2 + e3 + e4))\n",
    "\n",
    "            dr = self.decoder_resnet(e5)\n",
    "            dl = self.decoder_latent(x)\n",
    "\n",
    "            return dr * dl\n",
    "\n",
    "        def encode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.ln1(tf.keras.activations.swish(self.encoder1(e0)))\n",
    "            e2 = self.ln2(tf.keras.activations.swish(self.encoder2(e1) + e1))\n",
    "            e3 = self.ln3(tf.keras.activations.swish(self.encoder3(e2) + e1 + e2))\n",
    "            e4 = self.ln4(tf.keras.activations.swish(self.encoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.ln5(tf.keras.activations.swish(self.encoder5(e4) + e1 + e2 + e3 + e4))\n",
    "            e6 = self.ln6(tf.keras.activations.swish(self.encoder6(e5) + e1 + e2 + e3 + e4 + e5))\n",
    "            e7 = self.ln7(tf.keras.activations.swish(self.encoder7(e6) + e1 + e2 + e3 + e4 + e5 + e6))\n",
    "\n",
    "            z_mean = self.dense_mean(e7)\n",
    "            z_log_var = self.dense_log_var(e7)\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "            return z_mean, z_log_var, z\n",
    "\n",
    "    def create_model(self, latent=2048, hidden=4096, ease_items_sampling=1., summary=False):\n",
    "        self.model = VASP.Model(self.dataset.num_words, latent, hidden, ease_items_sampling)\n",
    "        self.model(self.split.train_gen[0][0])\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "        self.mc = MetricsCallback(self)\n",
    "\n",
    "    def compile_model(self, lr=0.00002, fl_alpha=0.25, fl_gamma=2.0):\n",
    "        \"\"\"\n",
    "        lr         learning rate of Nadam optimizer\n",
    "        fl_alpha   alpha parameter of focal crossentropy\n",
    "        fl_gamma   gamma parameter of focal crossentropy\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Nadam(lr),\n",
    "            loss=lambda x, y: tfa.losses.sigmoid_focal_crossentropy(x, y, alpha=fl_alpha, gamma=fl_gamma),\n",
    "            metrics=['mse', cosine_loss]\n",
    "        )\n",
    "\n",
    "    def train_model(self, epochs=150):\n",
    "        self.model.fit(\n",
    "            self.split.train_gen,\n",
    "            validation_data=self.split.validation_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=[self.mc]\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-ambassador",
   "metadata": {},
   "source": [
    "Now, we can load previously preprocessed dataset. We also load pre-defined train/test/validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "medium-literacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 0.025885581970214844\n",
      "Tokenizer trained for 2120 items.\n",
      "Creating 1 splits of 1000 samples each.\n",
      "Creating split nr. 1\n",
      "SplitGenerator init done in 0.016949176788330078 secs.\n",
      "SplitGenerator init done in 0.003221750259399414 secs.\n",
      "SplitGenerator init done in 0.00396728515625 secs.\n",
      "Creating evaluator\n",
      "Creating test split evaluator with leave_random_20_pct_out method.\n",
      "Creating validation split evaluator with leave_random_20_pct_out method.\n"
     ]
    }
   ],
   "source": [
    "dataset = Data(d=data_path, pruning='u5')\n",
    "dataset.splits = []\n",
    "dataset.create_splits(1, 1000, shuffle=False, n_fold=False, generators=False)\n",
    "dataset.split.train_users = pd.read_json(os.path.join(data_path, \"train_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.validation_users = pd.read_json(os.path.join(data_path, \"val_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.test_users = pd.read_json(os.path.join(data_path, \"test_users.json\")).userid.apply(str).to_frame()\n",
    "dataset.split.generators()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-handling",
   "metadata": {},
   "source": [
    "Now we can create an instance of our model and train it.\n",
    "\n",
    "For validation during training we sample 80% of the user's interactions randomly as input for the model, and then we measure Recall@20, Recall@50 and NDCG@100 for predicted interactions against the remaining 20% of the user's interactions.\n",
    "\n",
    "This method can give different results for different seeds, but since it is used for validation during training only, it's good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intellectual-mumbai",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vasp/custom_VASP.ipynb Zelle 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m VASP(dataset\u001b[39m.\u001b[39msplit, name\u001b[39m=\u001b[39mmodel_name)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m m\u001b[39m.\u001b[39;49mcreate_model(latent\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m, hidden\u001b[39m=\u001b[39;49m\u001b[39m4096\u001b[39;49m, ease_items_sampling\u001b[39m=\u001b[39;49m\u001b[39m0.33\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msummary()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m80\u001b[39m)\n",
      "\u001b[1;32m/home/ubuntu/vasp/custom_VASP.ipynb Zelle 10\u001b[0m in \u001b[0;36mVASP.create_model\u001b[0;34m(self, latent, hidden, ease_items_sampling, summary)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_model\u001b[39m(\u001b[39mself\u001b[39m, latent\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, hidden\u001b[39m=\u001b[39m\u001b[39m4096\u001b[39m, ease_items_sampling\u001b[39m=\u001b[39m\u001b[39m1.\u001b[39m, summary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m VASP\u001b[39m.\u001b[39mModel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mnum_words, latent, hidden, ease_items_sampling)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit\u001b[39m.\u001b[39;49mtrain_gen[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m     \u001b[39mif\u001b[39;00m summary:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bssm_ssh/home/ubuntu/vasp/custom_VASP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/vasp/utils.py:392\u001b[0m, in \u001b[0;36mSplitGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    389\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__len__\u001b[39m()))\n\u001b[1;32m    390\u001b[0m indices \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m indices\n\u001b[0;32m--> 392\u001b[0m index2 \u001b[39m=\u001b[39m indices[index \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\n\u001b[1;32m    393\u001b[0m index3 \u001b[39m=\u001b[39m indices[index \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m    394\u001b[0m index4 \u001b[39m=\u001b[39m indices[index \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "m = VASP(dataset.split, name=model_name)\n",
    "m.create_model(latent=2048, hidden=4096, ease_items_sampling=0.33)\n",
    "m.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for 50 epochs with lr 0.00005\")\n",
    "m.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(1)\n",
    "#print(\"=\" * 80)\n",
    "#print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "#m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "#m.train_model(20)\n",
    "#print(\"=\" * 80)\n",
    "#print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "#m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "#m.train_model(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-fantasy",
   "metadata": {},
   "source": [
    "Dataframe with the details of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mc.get_history_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-collins",
   "metadata": {},
   "source": [
    "And the details of the training as a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mc.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-payroll",
   "metadata": {},
   "source": [
    "For final evaluation we do 5-fold validation on the user's interaction history. We think that this is more objective measure than only hide sampled 20% of the user's interactions randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold evaluation on the test set\n",
    "\n",
    "test_r20s = []\n",
    "test_r50s = []\n",
    "test_n100s = []\n",
    "\n",
    "for fold in range(1,6):\n",
    "    ev=Evaluator(m.split, method=str(fold)+'_20')\n",
    "    ev.update(m.model)\n",
    "\n",
    "    test_n100s.append(ev.get_ncdg(100))\n",
    "    test_r20s.append(ev.get_recall(20))\n",
    "    test_r50s.append(ev.get_recall(50))\n",
    "\n",
    "print(\"TEST SET (MEAN)\")\n",
    "print(\"5-fold mean NCDG@100\", round(sum(test_n100s) / len(test_n100s),3))\n",
    "print(\"5-fold mean Recall@20\", round(sum(test_r20s) / len(test_r20s),3))\n",
    "print(\"5-fold mean Recall@50\", round(sum(test_r50s) / len(test_r50s),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
